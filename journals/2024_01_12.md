- #studynotes Machine Learning for Engineers
- **Lecture 4 - Hidden Markov Models**
	- these models add transition information between the Gaussians
	- ![image.png](../assets/image_1705051176006_0.png){:height 300, :width 300}
	- transition matrix describes the probability of getting from *state i* to *state j* in one step
	- applications of HMM -- used in many fields as a tool for time series or sequence analysis, and to recover a data sequence that is not immediately observable
		- sentence completion (ie. on mobile devices)
		- data compression
		- text classification
		- automatic writing
		- speech recognition
		- ![image.png](../assets/image_1705052270678_0.png){:height 400, :width 400}
	- Intermediary variables in HMM
		- ![image.png](../assets/image_1705054087952_0.png){:height 400, :width 400}
	- Viterbi decoding
		- MAP = maximum a posteriori
		- MPE = most probable explanation
		- if you're interested in knowing the most probable sequence of states
	- HMM with dynamic features
		- good for generating new data
- **Lecture 5 - Dimensionality Reduction**
	- why do we want dimensionality reduction?
		- data compression
		- usually for visualisation
			- ie. representing a face or a hand (or an element in a photo) accurately with LESS data --> faster comparison, less storage, allow interpretability (ie. what is the structure in the high dimensional space), etc.
	- PCA -- principal component analysis
		- manifold reduction technique
		- try to learn explicit or implicit mapping from the data
		- goals:
			- project data from space of dimension D into lower dimension space M *(M<D)*
		- principles:
			- variance maximization
			- reconstruction error minimization
		- Variance Maximization
		- Reconstruction Error Minimization
		- data whitening -- making the data more gaussian -- normalizing the data
		- **SUMMARY ABOUT PCA:**
			- interest
				- curse of dimensionality: high-dimensionality data difficult to manipulate
				- intrinsic data dimension is usually small
			- PCA
				- feature reduction technique, unsupervised (no data label)
				- project initial data points with a linear projection
				- projection directions given by eigenvectors of covariance matrix
				- projected point keep maximum variance of initial training points
			- application
				- data compression: less coordinates needed, therefore more efficient storage
				- visualization: project high-dim point into 2D or 3D
				- synthesis of new data point feasible
				- noise removal: keep only the essential information --> positive effect on subsequent steps
			- PCA is a way to remove correlation between points and reduce dimensions through linear projection
	- Probabilistic PCA
		- finding an even lower dimensional description of the data
		- advantage is that we first  present the model and then show its added properties
		- generative process: from latent (low dimensional space) to data space
		- **if all probabilistic distribution of graphical models are gaussian, than any involved distribution is gaussian as well!**