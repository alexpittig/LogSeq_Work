- #studynotes Machine Learning for Engineers
- **Lecture 4 - Hidden Markov Models**
	- these models add transition information between the Gaussians
	- ![image.png](../assets/image_1705051176006_0.png){:height 300, :width 300}
	- transition matrix describes the probability of getting from *state i* to *state j* in one step
	- applications of HMM -- used in many fields as a tool for time series or sequence analysis, and to recover a data sequence that is not immediately observable
		- sentence completion (ie. on mobile devices)
		- data compression
		- text classification
		- automatic writing
		- speech recognition
		- ![image.png](../assets/image_1705052270678_0.png){:height 400, :width 400}
	- Intermediary variables in HMM
		- ![image.png](../assets/image_1705054087952_0.png){:height 400, :width 400}
	- Viterbi decoding
		- MAP = maximum a posteriori
		- MPE = most probable explanation
		- if you're interested in knowing the most probable sequence of states
	- HMM with dynamic features
		- good for generating new data
- **Lecture 5 - Dimensionality Reduction**
	- why do we want dimensionality reduction?
		- data compression
		- usually for visualisation
			- ie. representing a face or a hand (or an element in a photo) accurately with LESS data --> faster comparison, less storage, allow interpretability (ie. what is the structure in the high dimensional space), etc.
	- PCA -- principal component analysis
		- manifold reduction technique
		- try to learn explicit or implicit mapping from the data
		- goals:
			- project data from space of dimension D into lower dimension space M *(M<D)*
		- principles:
			- variance maximization
			- reconstruction error minimization
		- Variance Maximization
		- Reconstruction Error Minimization
		- data whitening -- making the data more gaussian -- normalizing the data
		- **SUMMARY ABOUT PCA:**
			- interest
				- curse of dimensionality: high-dimensionality data difficult to manipulate
				- intrinsic data dimension is usually small
			- PCA
				- feature reduction technique, unsupervised (no data label)
				- project initial data points with a linear projection
				- projection directions given by eigenvectors of covariance matrix
				- projected point keep maximum variance of initial training points
			- application
				- data compression: less coordinates needed, therefore more efficient storage
				- visualization: project high-dim point into 2D or 3D
				- synthesis of new data point feasible
				- noise removal: keep only the essential information --> positive effect on subsequent steps
			- PCA is a way to remove correlation between points and reduce dimensions through linear projection
	- Probabilistic PCA
		- constrained form of Gaussian distribution on *p(x)*
		- classical probabilistic method for finding low-dimensional representation of the data
		- advantage is that we first  present the model and then show its added properties
		- generative process: from latent (low dimensional space) to data space
		- **if all probabilistic distribution of graphical models are gaussian, than any involved distribution is gaussian as well!**
		- Advantages of Probabilistic PCA vs. PCA
			- handles missing components in the data
			- number of free parameters is restricted (compared to full covariance model)
			- still captures dominant data correlation (compared to diagonal covariance model)
			- generative process (we can sample random vectors)
			- likelihood function of data points
				- more information than the PCA reconstruction error
				- allow comparison with other probability density models
			- latent space model -- derivation of computationally efficient EM algorithm for PCA, that does not require computation of covariance matrix
			- probabilistic model + EM -- handling of missing values in the dataset (allows PCA projections even if there are missing values in the input values)
			- principled extensions to other models and particularly mixtures of probabilistic PCA models
		- Factor Analysis (FA)
			- covariance is still taken diagonal, to that the components of x are all independent conditioned on z
			- no closed-form solution for the estimation of parameters
	- t-Stochastique-Neighbour-Embedding (t-SNE)
		- another dimensionality reduction algorithm
		- preserve local structure
		- low dimensional neighbourhood should be the same as original neighbourhood
		- one example of local embedding methods
		- used mainly for data visualisation
		- SNE (without the t) issues
			- crowding problem
				- high dimension space - more room, easy to have multiple neighbours
				- low dimensional space - area available to accommodate moderately distant point not large enough compared to area for nearby data points
				- distinct cluster in high dimensional space pushed closer in lower space -- might not be distinguishable
		- t-SNE
			- symmetrized distribution
				- less sensitive to outliers
				- leads to simpler gradient
			- use of a heavy-tail distribution
				- goes slower to 0 than Gaussian
				- slower change --> more space to move points around in medium distance
			- important parameters -- different perplexity captures different scales in data
				- ![image.png](../assets/image_1705062103738_0.png){:height 500, :width 500}
			- ![image.png](../assets/image_1705062264154_0.png){:height 500, :width 500}
			-